{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqrehhLnKRcv"
      },
      "source": [
        "In word embeddings, every word is represented as an n-dimensional dense vector. The words that are similar will have similar vector. Word embeddings techniques such as GloVe and Word2Vec have proven to be extremely efficient for converting words into corresponding dense vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiJ261AJKTeA"
      },
      "outputs": [],
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrpbPCU9Mp_h"
      },
      "outputs": [],
      "source": [
        "# parameters are vocabulary or total number of unique words in corpus, dimensions for each word vector, length of the input sentence\n",
        "embedding_layer = Embedding(200, 32, input_length=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Word Embedding"
      ],
      "metadata": {
        "id": "qvpX17DM2b1_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY8-VIDqL81R"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    # Positive Reviews\n",
        "\n",
        "    'This is an excellent movie',\n",
        "    'The move was fantastic I like it',\n",
        "    'You should watch it is brilliant',\n",
        "    'Exceptionally good',\n",
        "    'Wonderfully directed and executed I like it',\n",
        "    'Its a fantastic series',\n",
        "    'Never watched such a brillent movie',\n",
        "    'It is a Wonderful movie',\n",
        "\n",
        "    # Negtive Reviews\n",
        "\n",
        "    \"horrible acting\",\n",
        "    'waste of money',\n",
        "    'pathetic picture',\n",
        "    'It was very boring',\n",
        "    'I did not like the movie',\n",
        "    'The movie was horrible',\n",
        "    'I will not recommend',\n",
        "    'The acting is pathetic'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the word embedding is a 2D vector where words are represented in rows, whereas their corresponding dimensions are presented in columns. Finally, if you wish to directly connect your word embedding layer with a densely connected layer, you first have to flatten your 2D word embeddings into 1D. "
      ],
      "metadata": {
        "id": "bt8fCCDO18Ty"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_OSOd3IM3T0"
      },
      "outputs": [],
      "source": [
        "sentiments = array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euYezNR6OeFM",
        "outputId": "ece6d30e-5b0a-404f-dccd-d7372db4778a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eooTzYzZNEo-"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "all_words = []\n",
        "for sent in corpus:\n",
        "    tokenize_word = word_tokenize(sent)\n",
        "    for word in tokenize_word:\n",
        "        all_words.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1wrUDzLOgn9",
        "outputId": "212132d2-221d-4d27-9459-5a95ab6aa63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45\n"
          ]
        }
      ],
      "source": [
        "unique_words = set(all_words)\n",
        "print(len(unique_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKjw_1bKO8FF"
      },
      "outputs": [],
      "source": [
        "vocab_length = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Embedding layer expects the words to be in numeric form. Therefore, we need to convert the sentences in our corpus to numbers. One way to convert text to numbers is by using the one_hot function from the keras.preprocessing.text library. The function takes sentence and the total length of the vocabulary and returns the sentence in numeric form."
      ],
      "metadata": {
        "id": "X48gX8007Zs6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M_Rf-R8Okqc",
        "outputId": "2629fea0-e109-4e9c-f58e-8b57bda41805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7, 45, 12, 19, 47], [22, 31, 47, 49, 2, 21, 14], [7, 42, 1, 14, 45, 23], [48, 6], [46, 32, 27, 22, 2, 21, 14], [25, 49, 49, 17], [29, 34, 32, 49, 23, 47], [14, 45, 49, 43, 47], [12, 8], [9, 3, 30], [13, 9], [14, 47, 22, 27], [2, 13, 18, 21, 22, 47], [22, 47, 47, 12], [2, 6, 18, 17], [22, 8, 45, 13]]\n"
          ]
        }
      ],
      "source": [
        "embedded_sentences = [one_hot(sent, vocab_length) for sent in corpus]\n",
        "print(embedded_sentences )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding layer expects sentences to be of equal size. However, our encoded sentences are of different sizes. One way to make all the sentences of uniform size is to increase the lenght of all the sentences and make it equal to the length of the largest sentence. Let's first find the largest sentence in our corpus and then we will increase the length of all the sentences to the length of the largest sentence."
      ],
      "metadata": {
        "id": "7hYuaSB69oB7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WS0N56yO_nH"
      },
      "outputs": [],
      "source": [
        "word_count = lambda sentence: len(word_tokenize(sentence))\n",
        "longest_sentence = max(corpus, key=word_count)\n",
        "length_long_sentence = len(word_tokenize(longest_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we use a lambda expression to find the length of all the sentences. We then use the max function to return the longest sentence. Finally the longest sentence is tokenized into words and the number of words are counted using the len function.\n",
        "To make all the sentences of equal size, we will add zeros to the empty indexes that will be created as a result of increasing the sentence length. To append the zeros at the end of the sentencses, we can use the pad_sequences method. "
      ],
      "metadata": {
        "id": "gm2FYm1I-O6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8Titc4GPRtm",
        "outputId": "847b9749-e462-4d90-ddb6-70135aa26bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 7 45 12 19 47  0  0]\n",
            " [22 31 47 49  2 21 14]\n",
            " [ 7 42  1 14 45 23  0]\n",
            " [48  6  0  0  0  0  0]\n",
            " [46 32 27 22  2 21 14]\n",
            " [25 49 49 17  0  0  0]\n",
            " [29 34 32 49 23 47  0]\n",
            " [14 45 49 43 47  0  0]\n",
            " [12  8  0  0  0  0  0]\n",
            " [ 9  3 30  0  0  0  0]\n",
            " [13  9  0  0  0  0  0]\n",
            " [14 47 22 27  0  0  0]\n",
            " [ 2 13 18 21 22 47  0]\n",
            " [22 47 47 12  0  0  0]\n",
            " [ 2  6 18 17  0  0  0]\n",
            " [22  8 45 13  0  0  0]]\n"
          ]
        }
      ],
      "source": [
        "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
        "print(padded_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create a very simple text classification model with an embedding layer and no hidden layers. \n",
        "we create a Sequential model and add the Embedding layer as the first layer to the model. The length of the vocabulary is specified by the vocab_length parameter. The dimension of each word vector will be 20 and the input_length will be the length of the longest sentence, which is 7. Next, the Embedding layer is flattened so that it can be directly used with the densely connected layer. Since it is a binary classification problem, we use the sigmoid function as the loss function at the dense layer."
      ],
      "metadata": {
        "id": "_xxdrzFG-bcp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLPetDioPeTR"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_length, 20, input_length=length_long_sentence))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_8UeWBUPkq8",
        "outputId": "acbe2890-0048-40ed-a468-b1fd0635caf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 7, 20)             1000      \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 140)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 141       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,141\n",
            "Trainable params: 1,141\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the first layer has 1000 trainable parameters. This is because our vocabulary size is 50 and each word will be presented as a 20 dimensional vector. Hence the total number of trainable parameters will be 1000. Similarly, the output from the embedding layer will be a sentence with 7 words where each word is represented by a 20 dimensional vector. However, when the 2D output is flattened, we get a 140 dimensional vector (7 x 20). The flattened vector is directly connected to the dense layer that contains 1 neuran."
      ],
      "metadata": {
        "id": "NhBhNW3V_FNf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR_CtiynPtIj",
        "outputId": "d382a1fa-419b-4303-b8e5-7abaf52c27fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.6859 - acc: 0.5625\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6822 - acc: 0.6250\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6784 - acc: 0.6250\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6747 - acc: 0.6875\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6709 - acc: 0.6875\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6672 - acc: 0.6875\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6634 - acc: 0.8750\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6597 - acc: 0.8750\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6559 - acc: 0.8750\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6521 - acc: 0.9375\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6483 - acc: 0.9375\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6445 - acc: 0.9375\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6407 - acc: 0.9375\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6369 - acc: 0.9375\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6331 - acc: 0.9375\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6292 - acc: 0.9375\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6253 - acc: 0.9375\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6214 - acc: 0.9375\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6174 - acc: 0.9375\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6135 - acc: 0.9375\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6095 - acc: 0.9375\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6055 - acc: 0.9375\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6014 - acc: 0.9375\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5974 - acc: 0.9375\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5932 - acc: 0.9375\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5891 - acc: 0.9375\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5849 - acc: 0.9375\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5807 - acc: 0.9375\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5765 - acc: 0.9375\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5722 - acc: 0.9375\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5679 - acc: 0.9375\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5636 - acc: 0.9375\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5593 - acc: 0.9375\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5549 - acc: 0.9375\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5504 - acc: 0.9375\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5460 - acc: 0.9375\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5415 - acc: 0.9375\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5370 - acc: 0.9375\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5324 - acc: 0.9375\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5279 - acc: 0.9375\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5233 - acc: 0.9375\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5187 - acc: 0.9375\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5140 - acc: 0.9375\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5093 - acc: 0.9375\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5046 - acc: 0.9375\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4999 - acc: 0.9375\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4952 - acc: 0.9375\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4904 - acc: 0.9375\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4857 - acc: 0.9375\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4809 - acc: 0.9375\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4761 - acc: 0.9375\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4713 - acc: 0.9375\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4665 - acc: 0.9375\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4616 - acc: 0.9375\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4568 - acc: 0.9375\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4520 - acc: 0.9375\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4471 - acc: 0.9375\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4423 - acc: 0.9375\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4374 - acc: 0.9375\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4326 - acc: 0.9375\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4277 - acc: 0.9375\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4229 - acc: 0.9375\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4181 - acc: 0.9375\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4133 - acc: 0.9375\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4085 - acc: 0.9375\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4037 - acc: 0.9375\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3989 - acc: 0.9375\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3941 - acc: 0.9375\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3894 - acc: 0.9375\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3847 - acc: 0.9375\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3800 - acc: 0.9375\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3753 - acc: 0.9375\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3706 - acc: 0.9375\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3660 - acc: 0.9375\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3614 - acc: 0.9375\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3568 - acc: 0.9375\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3523 - acc: 0.9375\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3477 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3433 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3388 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3344 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3300 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3256 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3213 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3171 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3128 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3086 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3045 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3004 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2963 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2922 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2883 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2843 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2804 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2765 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2727 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2690 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2652 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2615 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2579 - acc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fce3613c1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "model.fit(padded_sentences, sentiments, epochs=100, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ze7j3jYP3Qo",
        "outputId": "42bd49dc-1c97-4c48-f374-b22c078a8de8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.000000\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkF6rtODQClJ"
      },
      "source": [
        "#Loading Pretrained Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz1orlvxVJVe"
      },
      "outputs": [],
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ljivLRtMVwlw",
        "outputId": "4a8b9e11-0e4c-4534-f2d6-76aeb83c320d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-41055531-ec96-493b-97db-851194cfbb85\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-41055531-ec96-493b-97db-851194cfbb85\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving glove.6B.100d.txt to glove.6B.100d.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-fRksEbVKYL"
      },
      "outputs": [],
      "source": [
        "embeddings_dictionary = dict()\n",
        "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "# from keras.layers.embeddings import Embedding"
      ],
      "metadata": {
        "id": "tb8_huMQUJfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    # Positive Reviews\n",
        "\n",
        "    'This is an excellent movie',\n",
        "    'The move was fantastic I like it',\n",
        "    'You should watch it is brilliant',\n",
        "    'Exceptionally good',\n",
        "    'Wonderfully directed and executed I like it',\n",
        "    'Its a fantastic series',\n",
        "    'Never watched such a brillent movie',\n",
        "    'It is a Wonderful movie',\n",
        "\n",
        "    # Negtive Reviews\n",
        "\n",
        "    \"horrible acting\",\n",
        "    'waste of money',\n",
        "    'pathetic picture',\n",
        "    'It was very boring',\n",
        "    'I did not like the movie',\n",
        "    'The movie was horrible',\n",
        "    'I will not recommend',\n",
        "    'The acting is pathetic'\n",
        "]"
      ],
      "metadata": {
        "id": "t94OWJRbUch9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiments = array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
      ],
      "metadata": {
        "id": "kSbKZu3yUkDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH7l-fAtU2vQ",
        "outputId": "91536381-95d3-4f70-a737-10413ff74721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "xcAm9oXdVYyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we used one_hot function to convert text to vectors. Another approach is to use Tokenizer function from keras.preprocessing.text library."
      ],
      "metadata": {
        "id": "oFM-2yD9Wyhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts(corpus)\n"
      ],
      "metadata": {
        "id": "GQQHXJPcUoB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the number of unique words in the text, you can simply count the length of word_index dictionary of the word_tokenizer object. add 1 with the vocabulary size. This is to store the dimensions for the words for which no pretrained word embeddings exist."
      ],
      "metadata": {
        "id": "smOM7w7yW6dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_length = len(word_tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "9yQ3kXTcV6Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to convert sentences to their numeric counterpart, call the texts_to_sequences function and pass it the whole corpus."
      ],
      "metadata": {
        "id": "NsuDK5-FXI0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_sentences = word_tokenizer.texts_to_sequences(corpus)\n",
        "print(embedded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYAKiCn2U54_",
        "outputId": "30b8a1f4-dcc2-4c07-bb07-c498c9bbf250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[14, 3, 15, 16, 1], [4, 17, 6, 9, 5, 7, 2], [18, 19, 20, 2, 3, 21], [22, 23], [24, 25, 26, 27, 5, 7, 2], [28, 8, 9, 29], [30, 31, 32, 8, 33, 1], [2, 3, 8, 34, 1], [10, 11], [35, 36, 37], [12, 38], [2, 6, 39, 40], [5, 41, 13, 7, 4, 1], [4, 1, 6, 10], [5, 42, 13, 43], [4, 11, 3, 12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "word_count = lambda sentence: len(word_tokenize(sentence))\n",
        "longest_sentence = max(corpus, key=word_count)\n",
        "length_long_sentence = len(word_tokenize(longest_sentence))\n",
        "\n",
        "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
        "\n",
        "print(padded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftoBWXWMUsje",
        "outputId": "39b90769-af8e-466b-e8e8-aa8423952a61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[14  3 15 16  1  0  0]\n",
            " [ 4 17  6  9  5  7  2]\n",
            " [18 19 20  2  3 21  0]\n",
            " [22 23  0  0  0  0  0]\n",
            " [24 25 26 27  5  7  2]\n",
            " [28  8  9 29  0  0  0]\n",
            " [30 31 32  8 33  1  0]\n",
            " [ 2  3  8 34  1  0  0]\n",
            " [10 11  0  0  0  0  0]\n",
            " [35 36 37  0  0  0  0]\n",
            " [12 38  0  0  0  0  0]\n",
            " [ 2  6 39 40  0  0  0]\n",
            " [ 5 41 13  7  4  1  0]\n",
            " [ 4  1  6 10  0  0  0]\n",
            " [ 5 42 13 43  0  0  0]\n",
            " [ 4 11  3 12  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "load the GloVe word embeddings and then create our embedding matrix that contains the words in our corpus and their corresponding values from GloVe embeddings.\n",
        "\n",
        "We will create a dictionary that will contain words as keys and the corresponding 100 dimensional vectors as values, in the form of an array."
      ],
      "metadata": {
        "id": "pnqiqq7XXrKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "\n",
        "glove_file.close()"
      ],
      "metadata": {
        "id": "hloQe-KOWO7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dictionary embeddings_dictionary now contains words and corresponding GloVe embeddings for all the words. \n",
        "\n",
        "word_tokenizer.word_index dictionary that contains our words and their corresponding index. \n",
        "\n",
        "Each word will be passed as key to the embedding_dictionary to retrieve the corresponding 100 dimensional vector for the word. The 100 dimensional vector will then be stored at the corresponding index of the word in the embedding_matrix"
      ],
      "metadata": {
        "id": "ilHEQ_PoYHvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = zeros((vocab_length, 100))\n",
        "for word, index in word_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "metadata": {
        "id": "BU1FpuBHWQks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "embedding_matrix now contains pretrained word embeddings for the words in the corpus"
      ],
      "metadata": {
        "id": "zrycJgOlYpT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "Z0pB42fpWVOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8Z0KcvnWYVb",
        "outputId": "b8f8eb75-9b91-4246-b86c-476333414cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 7, 100)            4400      \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 700)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 701       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,101\n",
            "Trainable params: 701\n",
            "Non-trainable params: 4,400\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "since we have 44 words in our vocabulary and each word will be represented as a 100 dimensional vector, the number of parameters for the embedding layer will be 44 x 100 = 4400. The output from the embedding layer will be a 2D vector with 7 rows (1 for each word in the sentence) and 100 columns. The output from the embedding layer will be flattened so that it can be used with the dense layer."
      ],
      "metadata": {
        "id": "iR4HT_r_bTgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(padded_sentences, sentiments, epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUXU0CZPUZR2",
        "outputId": "8f626b1c-9bcc-42a4-f35b-ea69326d858b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 441ms/step - loss: 0.7247 - acc: 0.6250\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6993 - acc: 0.6875\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6749 - acc: 0.6875\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6515 - acc: 0.6875\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6291 - acc: 0.6875\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6075 - acc: 0.6875\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5867 - acc: 0.6875\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5666 - acc: 0.6875\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5473 - acc: 0.8125\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5287 - acc: 0.8125\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5107 - acc: 0.8125\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4935 - acc: 0.8125\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4769 - acc: 0.8125\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4609 - acc: 0.8750\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4455 - acc: 0.9375\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4308 - acc: 0.9375\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4166 - acc: 0.9375\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4030 - acc: 0.9375\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3900 - acc: 0.9375\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3775 - acc: 0.9375\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3654 - acc: 0.9375\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3539 - acc: 0.9375\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3428 - acc: 0.9375\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3321 - acc: 1.0000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3219 - acc: 1.0000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3120 - acc: 1.0000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3026 - acc: 1.0000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2936 - acc: 1.0000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2849 - acc: 1.0000\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2766 - acc: 1.0000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2686 - acc: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2609 - acc: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2535 - acc: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2464 - acc: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2396 - acc: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2331 - acc: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2268 - acc: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2208 - acc: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2150 - acc: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2094 - acc: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2041 - acc: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1989 - acc: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1940 - acc: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1892 - acc: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1846 - acc: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1802 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1759 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1718 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1678 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1640 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1603 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1568 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1533 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1500 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1468 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1437 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1407 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1378 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1350 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1323 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1297 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1272 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1247 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1223 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1200 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1178 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1156 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1135 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1115 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1095 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1076 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1057 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1039 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1021 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1004 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0987 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0971 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0955 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0940 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0925 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0910 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0896 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0882 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0869 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0855 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0843 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0830 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0818 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0806 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0794 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0783 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0772 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0761 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0751 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0740 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0730 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0720 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0711 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0701 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0692 - acc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fce39fe4a50>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-UtwfpnUVTd",
        "outputId": "32b1b198-4b70-4b11-e535-4df09c323a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.000000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}